---
title: "IODS-Final"
author: "Ville Aro, ville.t.aro@helsinki.fi"
date: March 1, 2017
output:
  html_document:
    theme: united
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 8
    fig_height: 6
    code_folding: hide
---

```{r, include=FALSE}
library(ggplot2)
library(GGally)
library(MASS)
library(corrplot)
library(tidyverse)
library(plotly)
library(dplyr)
library(FactoMineR)
require(ggplot2)
library(corrplot)
```

# Abstract 
Analysing the grades in Student Alcohol Consumptions dataset with the idea that the final grade G3 could be explained with the first two grades G1 and G2 using clustering.

# Introduction and a brief look at my data

I am using a dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION). The dataset used in my analysis has combined information about the alcohol consumption of students of both maths and Portugese. This time I decided to take only the grades that the studetns got from the exams as I had last time analysed the alcohol consumption. Below are the variables and also explanation of the variable and how it was measured.

Variable | Description of variable and how it was measured
--------- | -----------------------------------------
G1 | first period grade (numeric: from 0 to 20) 
G2 | second period grade (numeric: from 0 to 20) 
G3 | final grade (numeric: from 0 to 20) 


Link to my [Data Wrangling](https://github.com/villetaro/IODS-Final/blob/master/alcdata.R)


After this we load the data.

```{r}
alc <- read.table("~/GitHub/IODS-Final/alcdata1", header=T, sep=",")

alcdata_ <- select(alc, -X)

dim(alcdata_)

colnames(alcdata_)

```


```{r}
summary(alcdata_)

```

# Graphical overview


```{r, include= FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

```{r}
g1 <- ggplot(alcdata_, aes(G1)) + geom_bar()+ theme_grey() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10))
g2 <- ggplot(alcdata_, aes(G2)) + geom_bar()+ theme_grey() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10))
g3 <- ggplot(alcdata_, aes(G3)) + geom_bar()+ theme_grey() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10))



multiplot(g1, g2, g3, cols=3)
```

This is a multiplot of the variables. The most interesting part is that the amount of grade = 0 goes up a lot when looking at the changes from G1 to G2 and G3. Otherwise the graphs look a lot like each other and the means are the following:

Variable | Mean
--------- | -----------------------------------------
G1 | 10.86
G2 | 10.71
G3 | 10.39

It shows how the mean goes down just a little bit when looking at the change from G1 to G2 to G3.

Next I drew a correlation matrix that shows the relationship between the three variables. They all are highly correlated with each other because they are measured the same way and the grades don't actually change that much during the analysis.

```{r}
cor_matrix<-cor(alcdata_) 



cor_matrix %>% round(digits = 2)
wb <- c("white","black") 

corrplot(cor_matrix, order="hclust", addrect=2, bg="gold2", col=wb)
```


# My hypothesis

As my major is Economics I try to make a model that is used in the stock and bond markets. In the model the best guess for the next value is its' current value. In this particular case the current values are G1 and G2 and the next value is the Final grade G3. That means that I am explaining the final score (G3) with the first two grades (G1 + G2). After that I'm clustering and looking at the results of the cluster.

# The regression model

The only reason that I wanted to show this regression model is that it gives a clear view of the data and also an idea how the high correlation might affect the analysis in the future. 


```{r, message=FALSE}
p2 <- ggplot(alcdata_, aes(x=G1, y= G3)) + geom_point(col="deepskyblue1") + geom_smooth( col = "red2")

p2
```

The plot already shows how much the zero values affect the regression. I thought about taking the away but then the results would've been biased upwards.

```{r}

model5 <- lm(G1 ~ G3, alcdata_)
summary(model5)
plot(model5,  which=c(1), col = "turquoise2",  lwd = 3)
```

```{r, message=FALSE}
p3 <- ggplot(alcdata_, aes(x=G2, y= G3)) + geom_point(col="red2") + geom_smooth(col = "royalblue1")

p3
```

The same thing happens here that also happened in the first regression model: the zero values affect a lot. These pictures also show well how correlated the variables are. This might be a trouble in the future but we there isn't much to do a this point. After all the results are done then we can figure out if the model would be better with other variables that might have	explanation power.

```{r}
model6 <- lm(G2 ~ G3, alcdata_)
summary(model6)
plot(model6,  which=c(1), col = "turquoise2",  lwd = 3)
```

# The clustering model

In clustering the idea is that some of the observations are closer to each other compared to the others. This means that the data is in a way clustered and the idea is to try to find these clusters. 

It is quite simple to find clusters but I want to optimize the amount of clusters which I do below after first clustering with 10 clusters. After this we try to analyse what happens when there is new observations. Could we classify them also?


First we scale the data so that all the means are zero.

```{r}
alcdata_scaled <- scale(alcdata_)
summary(alcdata_scaled)

```


```{r}
alcdata_scaled <- as.data.frame(alcdata_scaled)
```

Then we make 4 groups for the variables. The groups are the following: low, med_low, med_high and high. This is because in clustering the point is to group different observations.

# G1 

```{r}
cutoffs <- quantile(alcdata_scaled$G1)

labels <- c("low", "med_low", "med_high", "high")

G1_category <- cut(alcdata_scaled$G1, breaks = cutoffs, include.lowest = TRUE, label = labels)

table(G1_category)
```

#G2

```{r}
cutoffs <- quantile(alcdata_scaled$G2)

labels <- c("low", "med_low", "med_high", "high")

G2_category <- cut(alcdata_scaled$G2, breaks = cutoffs, include.lowest = TRUE, label = labels)

table(G2_category)
```

#G3

```{r}
cutoffs <- quantile(alcdata_scaled$G3)

labels <- c("low", "med_low", "med_high", "high")

G3_category <- cut(alcdata_scaled$G3, breaks = cutoffs, include.lowest = TRUE, label = labels)

table(G3_category)
```

Now we drop the old G3 variable from the dataset. Then we divide the dataset to train and test sets, so that 80% of the data belongs to the train set.


```{r}
alcdata_scaled <- dplyr::select(alcdata_scaled, -G3)
alcdata_scaled <- data.frame(alcdata_scaled, G3_category)
str(alcdata_scaled)

```

```{r}
n <- nrow(alcdata_scaled)
eighty_perc <- sample(n, size = n * 0.8)
train <- alcdata_scaled[eighty_perc,]
test <- alcdata_scaled[-eighty_perc,]

```

Now we fit the linear discriminant analysis on the train set. We use the categorical G3 as the target variable and all the other variables (G1 and G2) in the dataset as predictor variables. This gives us an idea how G3 also known as the final grade interacts with the other variables.

```{r}
lda.fit <- lda(G3_category ~ ., data = train)
lda.fit

```

# Results
Now we draw the LDA biplot

```{r}
lda.arrows <- function(x, myscale = 0.5, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$G3_category)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)



```

The biggest observation is that those "low" values are quite far away from the others. After that we see how G1 and G2 arrows represent the direction and volume of the predictive grade variables.

Now we predict the classes with the LDA model on the test data. We can help our analysis by cross tabulating the results with the G3 categories from the test set.

```{r}

correct_classes <- test$G3_category
test <- dplyr::select(test, -G3_category)

```
I see that all of the groups are pretty well predicted as most of the observations are the same in correct and predicted. This reason might be the same thing that I said above: the variables are highly correlated with each other.

```{r}

lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)

```
Let's see how well the model actually worked. I made a little observation during this analysis that the model changes everytime when I ran the code as some of the numbers come from random variables. As you read this the number may look a little different but I try to get the overall look.

The number of total observations
```{r}

total <- c(10+4+22+1+2+15+2+4+17)
total

```

We have 77 observations.

The number of correct predictions in this instance.
```{r}


correct <- c(10+22+15+17)
correct

```

We had 64 correctly predicted observations.

```{r}

wrong <- c(77-64)
wrong

```
And 13 of the observations were incorrectly predicted.

The accuracy of our model is the following:

```{r}
ratio <- c(correct/total)
ratio

```
At this time the model did ok but as I ran the code a couple of times the accuracy was quite different (0.83 as I write this). If I would get everytime 83% accuracy the model would actually be pretty good.

#Distances

Now we reload the alc dataset and standardize it. Then we take a look at the distances between the variables.


```{r}
New_alc <- alcdata_

str(New_alc)

New_alc_scaled <- scale(New_alc) %>% as.data.frame()

str(New_alc_scaled)
```
Now we calculate the distances.

```{r}
# Euclidean distance matrix using set.seed()
set.seed(123)
dist_eu <- dist(New_alc_scaled)
summary(dist_eu)


```
This is based on the Euclidean measure. And above you see the summary of the findings. Now we can start the clustering with the amount of 10 clusters. 10 is a safe bet that nothing would go wrong, below I calculate the optimum amount of clusters.

```{r}

km <-kmeans(dist_eu, centers=10)
pairs(New_alc_scaled, col = km$cluster)

```

As there is 10 clusters at the moment it is really hard to see what actually happens in this plot. The reason is because I have 10 clusters but only 2 variables. Let's find out if the amount of variables is also a good amount of clusters.

Because of the picture being so hard to interpret we want to optimize the amount of clusters so we don't have to guess it and then we can take a look at the plot again.

#The optimal amount of clusters


```{r}
k_max <- 10
wss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")

```


It seems that the optimal amount of clusters is 2. So we run the cluster analysis again and then take a look at the plot.


```{r}
km <-kmeans(dist_eu, centers=2)
pairs(New_alc_scaled, col = km$cluster)

```

Next we perform LDA using the clusters as target classes. We include all the grades in the alc data in the LDA model.

```{r}

km <-kmeans(dist_eu, centers = 3)
lda.fits <- lda(km$cluster~., data = alcdata_scaled)
lda.fits

```

```{r}

plot(lda.fits, dimen = 2, col = classes) 
lda.arrows(lda.fits, myscale = 1)

```
Using 3 clusters for K-means analysis, the most influencial linear separators are of course G1 and G2 as they are our only variables. G1 especially holds a strong predicting power. This gives us an idea that they would be the best variables to predict new observations if those would appear.

There is quite a large group of number 3 in the top right corner even though the arrows point in the opposite direction.

#Final words

The biggest concern that I have after this analysis is that there is omitted variable bias as I only have the first two grades explaining the final grade. This is not a huge issue in clustering as in clustering the objective is to to group the observations in a way that makes them more similar to each other. On the other hand if this was a real regression model then this would be a lot more problematic.

The omitted variable bias could be fixed with new variables or control variables. They could be for example the wealth of the family or how much alcohol the student consumes.

